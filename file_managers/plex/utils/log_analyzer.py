"""
Log Analysis Utility for Media Reorganization

This module provides tools to analyze the detailed logs generated by the media
reorganization system for deep insights into performance, classification accuracy,
and decision patterns.
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
# Optional dependencies for advanced analysis
try:
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    HAS_VISUALIZATION = True
except ImportError:
    HAS_VISUALIZATION = False


@dataclass
class ClassificationEvent:
    """Represents a single classification event."""
    timestamp: datetime
    filename: str
    current_category: str
    method: str
    suggested_category: str
    confidence: float
    reasoning: str
    processing_time: Optional[float] = None


@dataclass
class MisplacedFileEvent:
    """Represents a misplaced file detection event."""
    timestamp: datetime
    filename: str
    current_category: str
    suggested_category: str
    confidence: float
    reasoning: str
    file_size: int
    file_path: str


@dataclass
class SessionAnalysis:
    """Complete analysis of a reorganization session."""
    session_id: str
    start_time: datetime
    end_time: datetime
    total_duration: timedelta
    total_files: int
    misplaced_files: int
    classification_stats: Dict
    performance_metrics: Dict
    classification_events: List[ClassificationEvent]
    misplaced_events: List[MisplacedFileEvent]


class LogAnalyzer:
    """Analyzes media reorganization logs for insights."""
    
    def __init__(self, logs_directory: str = "logs"):
        """Initialize the log analyzer."""
        self.logs_dir = Path(logs_directory)
        self.session_pattern = re.compile(r'media_reorganizer_(\d{8}_\d{6})\.log')
        
    def get_available_sessions(self) -> List[str]:
        """Get list of available session IDs."""
        sessions = []
        if self.logs_dir.exists():
            for log_file in self.logs_dir.glob("media_reorganizer_*.log"):
                match = self.session_pattern.match(log_file.name)
                if match:
                    sessions.append(match.group(1))
        return sorted(sessions, reverse=True)
    
    def analyze_session(self, session_id: str) -> Optional[SessionAnalysis]:
        """Analyze a specific session log."""
        log_file = self.logs_dir / f"media_reorganizer_{session_id}.log"
        if not log_file.exists():
            print(f"Log file not found: {log_file}")
            return None
        
        print(f"Analyzing session: {session_id}")
        
        with open(log_file, 'r', encoding='utf-8') as f:
            log_content = f.read()
        
        # Parse session metadata
        session_info = self._parse_session_info(log_content, session_id)
        
        # Parse classification events
        classification_events = self._parse_classification_events(log_content)
        
        # Parse misplaced file events
        misplaced_events = self._parse_misplaced_events(log_content)
        
        # Parse performance metrics
        performance_metrics = self._parse_performance_metrics(log_content)
        
        # Parse classification statistics
        classification_stats = self._parse_classification_stats(log_content)
        
        return SessionAnalysis(
            session_id=session_id,
            start_time=session_info['start_time'],
            end_time=session_info['end_time'],
            total_duration=session_info['duration'],
            total_files=session_info['total_files'],
            misplaced_files=len(misplaced_events),
            classification_stats=classification_stats,
            performance_metrics=performance_metrics,
            classification_events=classification_events,
            misplaced_events=misplaced_events
        )
    
    def _parse_session_info(self, log_content: str, session_id: str) -> Dict:
        """Parse basic session information."""
        lines = log_content.split('\n')
        
        start_time = None
        end_time = None
        total_files = 0
        
        for line in lines:
            if "Session Started:" in line:
                # Extract timestamp from log line
                timestamp_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                if timestamp_match:
                    start_time = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
            
            elif "Analysis session completed successfully" in line:
                timestamp_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                if timestamp_match:
                    end_time = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
            
            elif "Database loaded" in line and "files" in line:
                # Extract total files count
                match = re.search(r'(\d{1,3}(?:,\d{3})*) files', line)
                if match:
                    total_files = int(match.group(1).replace(',', ''))
        
        duration = end_time - start_time if start_time and end_time else timedelta(0)
        
        return {
            'start_time': start_time,
            'end_time': end_time,
            'duration': duration,
            'total_files': total_files
        }
    
    def _parse_classification_events(self, log_content: str) -> List[ClassificationEvent]:
        """Parse individual classification events."""
        events = []
        lines = log_content.split('\n')
        
        for line in lines:
            if "classification" in line.lower() and ("suggests move" in line or "high confidence" in line):
                timestamp_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                if timestamp_match:
                    timestamp = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                    
                    # Parse different classification event types
                    if "AI classification (high confidence)" in line:
                        # AI classification event
                        match = re.search(r'AI classification \(high confidence\): (.+?) -> (\w+) \((\d+\.\d+)\)', line)
                        if match:
                            events.append(ClassificationEvent(
                                timestamp=timestamp,
                                filename=match.group(1),
                                current_category="unknown",  # Not always available in this log line
                                method="AI",
                                suggested_category=match.group(2),
                                confidence=float(match.group(3)),
                                reasoning="High confidence AI classification"
                            ))
                    
                    elif "suggests move" in line:
                        # Rule-based classification suggesting move
                        match = re.search(r'suggests move: (.+?) from (\w+) to (\w+) \((\d+\.\d+)\)', line)
                        if match:
                            events.append(ClassificationEvent(
                                timestamp=timestamp,
                                filename=match.group(1),
                                current_category=match.group(2),
                                method="Rule-based",
                                suggested_category=match.group(3),
                                confidence=float(match.group(4)),
                                reasoning="Rule-based classification suggests move"
                            ))
        
        return events
    
    def _parse_misplaced_events(self, log_content: str) -> List[MisplacedFileEvent]:
        """Parse misplaced file detection events."""
        events = []
        lines = log_content.split('\n')
        
        i = 0
        while i < len(lines):
            line = lines[i]
            if "MISPLACED FILE DETECTED:" in line:
                timestamp_match = re.match(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                if timestamp_match:
                    timestamp = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                    
                    # Parse the following lines for file details
                    event_data = {}
                    for j in range(1, 7):  # Check next 6 lines for file details
                        if i + j < len(lines):
                            detail_line = lines[i + j]
                            
                            if "File:" in detail_line:
                                event_data['filename'] = detail_line.split("File: ")[1].strip()
                            elif "Current:" in detail_line and "Suggested:" in detail_line:
                                match = re.search(r'Current: (\w+) -> Suggested: (\w+)', detail_line)
                                if match:
                                    event_data['current_category'] = match.group(1)
                                    event_data['suggested_category'] = match.group(2)
                            elif "Confidence:" in detail_line:
                                match = re.search(r'Confidence: (\d+\.\d+)', detail_line)
                                if match:
                                    event_data['confidence'] = float(match.group(1))
                            elif "Reasoning:" in detail_line:
                                event_data['reasoning'] = detail_line.split("Reasoning: ")[1].strip()
                            elif "Size:" in detail_line:
                                size_text = detail_line.split("Size: ")[1].strip()
                                event_data['file_size'] = self._parse_file_size(size_text)
                            elif "Path:" in detail_line:
                                event_data['file_path'] = detail_line.split("Path: ")[1].strip()
                    
                    # Create event if we have enough data
                    if all(key in event_data for key in ['filename', 'current_category', 'suggested_category', 'confidence']):
                        events.append(MisplacedFileEvent(
                            timestamp=timestamp,
                            filename=event_data['filename'],
                            current_category=event_data['current_category'],
                            suggested_category=event_data['suggested_category'],
                            confidence=event_data['confidence'],
                            reasoning=event_data.get('reasoning', ''),
                            file_size=event_data.get('file_size', 0),
                            file_path=event_data.get('file_path', '')
                        ))
            i += 1
        
        return events
    
    def _parse_performance_metrics(self, log_content: str) -> Dict:
        """Parse performance timing information."""
        metrics = {}
        
        # Database loading time
        db_match = re.search(r'Database loaded in (\d+\.\d+) seconds', log_content)
        if db_match:
            metrics['database_loading_time'] = float(db_match.group(1))
        
        # Analysis time
        analysis_match = re.search(r'Analysis completed in (\d+\.\d+) seconds', log_content)
        if analysis_match:
            metrics['analysis_time'] = float(analysis_match.group(1))
        
        # Report generation time
        report_match = re.search(r'Reports generated in (\d+\.\d+) seconds', log_content)
        if report_match:
            metrics['report_generation_time'] = float(report_match.group(1))
        
        # Total time
        total_match = re.search(r'Total analysis time: (\d+\.\d+) seconds', log_content)
        if total_match:
            metrics['total_time'] = float(total_match.group(1))
        
        return metrics
    
    def _parse_classification_stats(self, log_content: str) -> Dict:
        """Parse classification statistics."""
        stats = {}
        
        # Look for the classification statistics section
        stats_section = re.search(r'CLASSIFICATION STATISTICS.*?={40}(.*?)={40}', log_content, re.DOTALL)
        if stats_section:
            stats_text = stats_section.group(1)
            
            # Parse individual statistics
            for line in stats_text.split('\n'):
                if "Total Classifications:" in line:
                    match = re.search(r'Total Classifications: (\d+)', line)
                    if match:
                        stats['total_classifications'] = int(match.group(1))
                
                elif "AI Classifications:" in line:
                    match = re.search(r'AI Classifications: (\d+) \(Success: (\d+)\)', line)
                    if match:
                        stats['ai_calls'] = int(match.group(1))
                        stats['ai_success'] = int(match.group(2))
                
                elif "API Classifications:" in line:
                    match = re.search(r'API Classifications: (\d+) \(Success: (\d+)\)', line)
                    if match:
                        stats['api_calls'] = int(match.group(1))
                        stats['api_success'] = int(match.group(2))
                
                elif "Rule-based Classifications:" in line:
                    match = re.search(r'Rule-based Classifications: (\d+)', line)
                    if match:
                        stats['rule_based'] = int(match.group(1))
                
                elif "AI Success Rate:" in line:
                    match = re.search(r'AI Success Rate: (\d+\.\d+)%', line)
                    if match:
                        stats['ai_success_rate'] = float(match.group(1))
                
                elif "Average AI Time:" in line:
                    match = re.search(r'Average AI Time: (\d+\.\d+)s', line)
                    if match:
                        stats['avg_ai_time'] = float(match.group(1))
                
                elif "Average API Time:" in line:
                    match = re.search(r'Average API Time: (\d+\.\d+)s', line)
                    if match:
                        stats['avg_api_time'] = float(match.group(1))
                
                elif "Average Rule Time:" in line:
                    match = re.search(r'Average Rule Time: (\d+\.\d+)s', line)
                    if match:
                        stats['avg_rule_time'] = float(match.group(1))
        
        return stats
    
    def _parse_file_size(self, size_text: str) -> int:
        """Parse file size from human-readable format to bytes."""
        size_text = size_text.strip()
        
        # Extract number and unit
        match = re.match(r'(\d+\.\d+)\s*(\w+)', size_text)
        if match:
            size_value = float(match.group(1))
            unit = match.group(2).upper()
            
            multipliers = {
                'B': 1,
                'KB': 1024,
                'MB': 1024**2,
                'GB': 1024**3,
                'TB': 1024**4
            }
            
            return int(size_value * multipliers.get(unit, 1))
        
        return 0
    
    def generate_analysis_report(self, session: SessionAnalysis, output_file: Optional[str] = None) -> str:
        """Generate a comprehensive analysis report."""
        report_lines = []
        
        # Header
        report_lines.append("="*80)
        report_lines.append(f"MEDIA REORGANIZATION LOG ANALYSIS REPORT")
        report_lines.append("="*80)
        report_lines.append(f"Session ID: {session.session_id}")
        report_lines.append(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")
        
        # Session Overview
        report_lines.append("SESSION OVERVIEW")
        report_lines.append("-" * 40)
        report_lines.append(f"Start Time: {session.start_time}")
        report_lines.append(f"End Time: {session.end_time}")
        report_lines.append(f"Duration: {session.total_duration}")
        report_lines.append(f"Total Files Processed: {session.total_files:,}")
        report_lines.append(f"Misplaced Files Found: {session.misplaced_files:,}")
        if session.total_files > 0:
            misplacement_rate = (session.misplaced_files / session.total_files) * 100
            report_lines.append(f"Misplacement Rate: {misplacement_rate:.2f}%")
        report_lines.append("")
        
        # Performance Metrics
        if session.performance_metrics:
            report_lines.append("PERFORMANCE METRICS")
            report_lines.append("-" * 40)
            for metric, value in session.performance_metrics.items():
                metric_name = metric.replace('_', ' ').title()
                report_lines.append(f"{metric_name}: {value:.2f} seconds")
            
            if session.total_files > 0 and 'analysis_time' in session.performance_metrics:
                files_per_second = session.total_files / session.performance_metrics['analysis_time']
                report_lines.append(f"Processing Rate: {files_per_second:.2f} files/second")
            report_lines.append("")
        
        # Classification Statistics
        if session.classification_stats:
            report_lines.append("CLASSIFICATION STATISTICS")
            report_lines.append("-" * 40)
            stats = session.classification_stats
            
            if 'total_classifications' in stats:
                report_lines.append(f"Total Classifications: {stats['total_classifications']:,}")
            
            if 'ai_calls' in stats:
                report_lines.append(f"AI Classifications: {stats['ai_calls']:,}")
                if 'ai_success_rate' in stats:
                    report_lines.append(f"  Success Rate: {stats['ai_success_rate']:.1f}%")
                if 'avg_ai_time' in stats:
                    report_lines.append(f"  Average Time: {stats['avg_ai_time']:.3f}s per file")
            
            if 'api_calls' in stats:
                report_lines.append(f"API Classifications: {stats['api_calls']:,}")
                if 'api_success' in stats and stats['api_calls'] > 0:
                    api_success_rate = (stats['api_success'] / stats['api_calls']) * 100
                    report_lines.append(f"  Success Rate: {api_success_rate:.1f}%")
                if 'avg_api_time' in stats:
                    report_lines.append(f"  Average Time: {stats['avg_api_time']:.3f}s per file")
            
            if 'rule_based' in stats:
                report_lines.append(f"Rule-based Classifications: {stats['rule_based']:,}")
                if 'avg_rule_time' in stats:
                    report_lines.append(f"  Average Time: {stats['avg_rule_time']:.3f}s per file")
            report_lines.append("")
        
        # Category Analysis
        if session.misplaced_events:
            report_lines.append("CATEGORY TRANSITION ANALYSIS")
            report_lines.append("-" * 40)
            
            # Count transitions
            transitions = {}
            total_size = 0
            
            for event in session.misplaced_events:
                transition = f"{event.current_category} → {event.suggested_category}"
                if transition not in transitions:
                    transitions[transition] = {'count': 0, 'size': 0}
                transitions[transition]['count'] += 1
                transitions[transition]['size'] += event.file_size
                total_size += event.file_size
            
            # Sort by count
            sorted_transitions = sorted(transitions.items(), key=lambda x: x[1]['count'], reverse=True)
            
            for transition, data in sorted_transitions:
                size_gb = data['size'] / (1024**3)
                report_lines.append(f"{transition}: {data['count']:,} files ({size_gb:.1f} GB)")
            
            total_size_gb = total_size / (1024**3)
            report_lines.append(f"Total Data to Reorganize: {total_size_gb:.1f} GB")
            report_lines.append("")
        
        # Confidence Distribution
        if session.misplaced_events:
            report_lines.append("CONFIDENCE DISTRIBUTION")
            report_lines.append("-" * 40)
            
            confidence_ranges = {
                "High (≥0.9)": 0,
                "Medium-High (0.8-0.89)": 0,
                "Medium (0.7-0.79)": 0,
                "Low (<0.7)": 0
            }
            
            for event in session.misplaced_events:
                if event.confidence >= 0.9:
                    confidence_ranges["High (≥0.9)"] += 1
                elif event.confidence >= 0.8:
                    confidence_ranges["Medium-High (0.8-0.89)"] += 1
                elif event.confidence >= 0.7:
                    confidence_ranges["Medium (0.7-0.79)"] += 1
                else:
                    confidence_ranges["Low (<0.7)"] += 1
            
            for range_name, count in confidence_ranges.items():
                percentage = (count / len(session.misplaced_events)) * 100 if session.misplaced_events else 0
                report_lines.append(f"{range_name}: {count:,} files ({percentage:.1f}%)")
            report_lines.append("")
        
        # Recommendations
        report_lines.append("RECOMMENDATIONS")
        report_lines.append("-" * 40)
        
        if session.classification_stats:
            if session.classification_stats.get('ai_success_rate', 0) < 50:
                report_lines.append("• Consider reviewing AI classification patterns - low success rate")
            
            if session.classification_stats.get('avg_ai_time', 0) > 3:
                report_lines.append("• AI processing is slow - consider reducing batch size or model optimization")
            
            if session.classification_stats.get('api_calls', 0) > 0 and session.classification_stats.get('api_success', 0) == 0:
                report_lines.append("• External API calls are failing - check API keys and connectivity")
        
        if session.misplaced_files > session.total_files * 0.1:
            report_lines.append("• High misplacement rate detected - review classification rules")
        
        if session.misplaced_files == 0:
            report_lines.append("• No misplaced files detected - collection is well organized")
        
        report_lines.append("")
        report_lines.append("="*80)
        
        report_text = "\n".join(report_lines)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"Analysis report saved to: {output_file}")
        
        return report_text
    
    def compare_sessions(self, session_ids: List[str]) -> str:
        """Compare multiple sessions for trend analysis."""
        sessions = []
        for session_id in session_ids:
            session = self.analyze_session(session_id)
            if session:
                sessions.append(session)
        
        if not sessions:
            return "No valid sessions found for comparison."
        
        report_lines = []
        report_lines.append("="*80)
        report_lines.append("SESSION COMPARISON REPORT")
        report_lines.append("="*80)
        
        # Summary table
        report_lines.append(f"{'Session ID':<17} {'Files':<8} {'Misplaced':<10} {'Rate %':<8} {'Duration':<10}")
        report_lines.append("-" * 70)
        
        for session in sessions:
            rate = (session.misplaced_files / session.total_files * 100) if session.total_files > 0 else 0
            duration = str(session.total_duration).split('.')[0]  # Remove microseconds
            report_lines.append(f"{session.session_id:<17} {session.total_files:<8,} {session.misplaced_files:<10,} {rate:<8.1f} {duration:<10}")
        
        return "\n".join(report_lines)


def main():
    """CLI interface for log analysis."""
    analyzer = LogAnalyzer()
    sessions = analyzer.get_available_sessions()
    
    if not sessions:
        print("No log files found in the logs directory.")
        return
    
    print("Available sessions:")
    for i, session_id in enumerate(sessions, 1):
        print(f"{i}. {session_id}")
    
    try:
        choice = input(f"\nSelect session to analyze (1-{len(sessions)}): ")
        session_index = int(choice) - 1
        
        if 0 <= session_index < len(sessions):
            session_id = sessions[session_index]
            session = analyzer.analyze_session(session_id)
            
            if session:
                # Generate analysis report
                output_file = f"analysis_report_{session_id}.txt"
                report = analyzer.generate_analysis_report(session, output_file)
                print("\n" + report)
            else:
                print("Failed to analyze session.")
        else:
            print("Invalid selection.")
    
    except (ValueError, KeyboardInterrupt):
        print("\nAnalysis cancelled.")


if __name__ == "__main__":
    main()